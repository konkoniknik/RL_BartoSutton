{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/konkoniknik/RL_BartoSutton/blob/main/RL_BartoSutton_Part1_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task: Perform some implementations of algos from Barto Sutton's Reinforcement Learning: An Introduction. These are implementations of Part I (i.e., non approximate mainly tabular solutions).\n",
        "\n",
        "For our toy experiments we implement a simple gridworld with obstacles and some squares where the episode ends\n"
      ],
      "metadata": {
        "id": "Kr5eHmP1SOHC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwpXmoRs28_t"
      },
      "source": [
        "# 0. The world"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s79ybGNQ6yfx",
        "outputId": "889a235c-4dd7-4d0c-aaad-52ff491e313b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.]\n",
            " [  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.]\n",
            " [  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.]\n",
            " [  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.]\n",
            " [  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.]\n",
            " [-100. -100. -100. -100. -100. -100. -100. -100.   -1.   -1.]\n",
            " [  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.]\n",
            " [  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.]\n",
            " [  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.    0.]\n",
            " [  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.    0.    0.]]\n",
            "[(0, 0)] [(9, 9), (9, 8), (8, 9), (8, 8)]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# We assume that for each action the relevant neighboring square has p(r,s\"; a,s )=0.8\n",
        "# and 0.2 for all other states. r=-1 everywhere except for the terminal state where it\n",
        "# is 0\n",
        "\n",
        "class world:\n",
        "\n",
        "  def __init__(self, matrix, gamma=0.95, p=1):\n",
        "    self.rewards = matrix\n",
        "    print(self.rewards)\n",
        "    self.gamma=gamma\n",
        "\n",
        "    self.p_env=p\n",
        "\n",
        "    self.s_start= [(0,0)]\n",
        "    self.s_terminal=[(matrix.shape[0]-1,matrix.shape[1]-1),(matrix.shape[0]-1,matrix.shape[1]-2),\n",
        "                     (matrix.shape[0]-2,matrix.shape[1]-1),(matrix.shape[0]-2,matrix.shape[1]-2)]\n",
        "\n",
        "    self.a_space =[(0,1),(0,-1),(1,0),(-1,0),(0,0)]\n",
        "    self.a_bounds=(0,0,9,9)\n",
        "\n",
        "\n",
        "\n",
        "m=np.zeros([10,10])\n",
        "m[:]=-1\n",
        "\n",
        "#m[2,:5]=-100\n",
        "m[5,:8]=-100\n",
        "\n",
        "#m[5:-2,5]=-100\n",
        "#m[4,5:]=-100\n",
        "\n",
        "m[-1,-1],m[-2,-2]=0,0\n",
        "m[-1,-2],m[-2,-1]=0,0\n",
        "\n",
        "\n",
        "\n",
        "w= world(matrix=m)\n",
        "\n",
        "\n",
        "print(w.s_start,w.s_terminal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaU4HGK0yIqc"
      },
      "source": [
        "###  General **Functions** and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tl6gUzmiyHl-"
      },
      "outputs": [],
      "source": [
        "# Print the evaluation\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def policy_changed(old_policy,new_policy, display_changes=False):\n",
        "   if not np.array_equal(old_policy,new_policy):\n",
        "        print(ii, \"policy changed:\")\n",
        "        if display_changes:\n",
        "          for i, row in enumerate(old_policy):\n",
        "            for j, e in enumerate(row):\n",
        "              if tuple(old_policy[i,j])!=tuple(new_policy[i,j]):\n",
        "                 print(f\"({i}_{j}): old:{old_policy[i,j]}-> {new_policy[i,j]}\")\n",
        "\n",
        "\n",
        "def calc_min_max_Q_n_greedy(Q, world, policy):\n",
        "   # calculate the max value per state\n",
        "\n",
        "   max_Q=np.zeros(world.rewards.shape)\n",
        "   min_Q=np.zeros(world.rewards.shape)\n",
        "\n",
        "   for i,row in enumerate(max_Q):\n",
        "    for j,_ in enumerate(row):\n",
        "      s=(i,j)\n",
        "      state_vals=[]\n",
        "      for a in world.a_space:\n",
        "        key=f\"{i}_{j}_{a[0]}_{a[1]}\"\n",
        "        cond= ((i==0) and (a==(-1,0))) or ((j==0) and (a==(0,-1))) or ((i==9) and (a==(1,0))) or ((j==9) and (a==(0,1)))\n",
        "\n",
        "        if not cond:\n",
        "          state_vals.append(Q[key])\n",
        "        else:\n",
        "          state_vals.append(-10000)\n",
        "\n",
        "      max_Q[s]=max(state_vals)\n",
        "      min_Q[s]=min(state_vals)# calculating for presentation of the progress on the heatmpas\n",
        "      chosen_index=state_vals.index(max_Q[s])\n",
        "      chosen_action = world.a_space[chosen_index]\n",
        "\n",
        "      policy[s]=chosen_action\n",
        "\n",
        "\n",
        "   return max_Q,policy,min_Q\n",
        "\n",
        "\n",
        "\n",
        "def create_Q_heatmap(max_Q, min_Q):\n",
        "\n",
        "  max_Q_rounded= np.round(max_Q,1)\n",
        "  min_Q_rounded= np.round(min_Q,1)\n",
        "\n",
        "  # Setting up the plotting environment\n",
        "  plt.figure(figsize=(16, 6))\n",
        "\n",
        "  # Creating the first heatmap\n",
        "  plt.subplot(1, 2, 1)  # (rows, columns, subplot number)\n",
        "  sns.heatmap(max_Q_rounded, annot=True, cmap='viridis')\n",
        "  plt.title('Heatmap of Array 1')\n",
        "\n",
        "  # Creating the second heatmap\n",
        "  plt.subplot(1, 2, 2)\n",
        "  sns.heatmap(min_Q_rounded, annot=False, cmap='magma')\n",
        "  plt.title('Heatmap of Array 2')\n",
        "\n",
        "  # Display the heatmaps\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def initialize_Q(world, zeros_flag=True):\n",
        "  Q={}\n",
        "  total_a_space=world.a_space#+[(0,0)]\n",
        "  for i,row in enumerate(world.rewards):\n",
        "    for j, _ in enumerate(row):\n",
        "      key=str(i)+\"_\"+str(j)\n",
        "      for a in total_a_space:\n",
        "          cond= ((i==0) and (a==(-1,0))) or ((j==0) and (a==(0,-1))) or ((i==9) and (a==(1,0))) or ((j==9) and (a==(0,1)))\n",
        "          a_key=\"_\"+str(a[0])+\"_\"+str(a[1])\n",
        "          if not cond:\n",
        "            Q[key+a_key]=0 if zeros_flag else np.random.randn()\n",
        "\n",
        "          # Q is 0 in terminal states\n",
        "          if (i,j) in world.s_terminal:\n",
        "            Q[key+a_key]=0\n",
        "\n",
        "  return Q\n",
        "\n",
        "\n",
        "def initialize_policy(world, init=\"random\"):\n",
        "  action_space_size=len(world.a_space)\n",
        "  length, width=w.rewards.shape[0]-1, w.rewards.shape[1]-1\n",
        "\n",
        "\n",
        "  if init==\"random\":\n",
        "    random_indices=np.random.choice(action_space_size, size=[length+1,width+1])\n",
        "\n",
        "    A= np.array(world.a_space)[random_indices]\n",
        "    for i,row in enumerate(A):\n",
        "      for j,e in enumerate(row):\n",
        "        if i==0 and tuple(A[i,j])==(-1,0):\n",
        "          A[i,j]=(1,0)\n",
        "\n",
        "        if j==0 and tuple(A[i,j])==(0,-1):\n",
        "          A[i,j]=(0,1)\n",
        "\n",
        "  if init==\"down\":\n",
        "    print(\"Down\")\n",
        "    A=np.full((world.rewards.shape[0],world.rewards.shape[1],2), (1,0))\n",
        "\n",
        "  return A\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Unused here: all probabilistic transitions need to be incorporated\n",
        "def calc_transition(world, policy,s, display=False, epsilon=0.1):\n",
        "  random_num= random.randint(1,100)/100\n",
        "  #print(policy[s], random_int)\n",
        "  # Epsilon - greedy\n",
        "  if random_num<=1-epsilon:\n",
        "    action=policy[s]\n",
        "  else:\n",
        "    tmp_spaces=world.a_space.copy()\n",
        "    tmp_spaces.remove(tuple(policy[s]))\n",
        "    random_num2=random.randint(0,len(tmp_spaces)-1)\n",
        "    action=tmp_spaces[random_num2]\n",
        "\n",
        "\n",
        "\n",
        "  new_s_tnt= tuple(a+b if (a+b<=9) and (a+b>=0) else a for a,b in zip(s, action))\n",
        "\n",
        "  random_int= random.randint(1,100)/100\n",
        "\n",
        "  if tuple(action)==(0,0):\n",
        "    new_s=new_s_tnt\n",
        "\n",
        "  # adding environments randomness\n",
        "  if tuple(action)==(0,1) or tuple(action)==(0,-1):\n",
        "    if random_int<=world.p_env:\n",
        "      new_s=new_s_tnt\n",
        "    elif random_int<=world.p_env+(1-world.p_env)/2:\n",
        "      new_s= tuple([new_s_tnt[0]+1 if 0<=new_s_tnt[0]+1<=9 else new_s_tnt[0], new_s_tnt[1]])\n",
        "    else:\n",
        "      new_s= tuple([new_s_tnt[0]-1 if 0<=new_s_tnt[0]-1<=9 else new_s_tnt[0], new_s_tnt[1]])\n",
        "\n",
        "  if tuple(action)==(1,0) or tuple(action)==(-1,0):\n",
        "    if random_int<=world.p_env:\n",
        "      new_s=new_s_tnt\n",
        "    elif random_int<=world.p_env+(1-world.p_env)/2:\n",
        "      new_s= tuple([new_s_tnt[0], new_s_tnt[1]+1 if 0<=new_s_tnt[1]+1<=9 else new_s_tnt[1]])\n",
        "    else:\n",
        "      new_s= tuple([new_s_tnt[0], new_s_tnt[1]-1 if 0<=new_s_tnt[1]-1<=9 else new_s_tnt[1]])\n",
        "\n",
        "  if display:\n",
        "    print(\"old s\", s, \"new s\", new_s, \"policy s\",policy[s], \"action\",action,\"random env:\", random_int, \"random num\", random_num, \"epsilon\", epsilon)\n",
        "\n",
        "  return new_s, action, world.rewards[new_s]\n",
        "\n",
        "\n",
        "\n",
        "def policy_visualization(policy):\n",
        "  # Create a simple representation for a policy\n",
        "  policy_vis=np.full((10,10), \"\")\n",
        "  transform={(1,0):\"V\", (-1,0):\"A\", (0,1):\">\",(0,-1):\"<\", (0,0):\"O\"}\n",
        "  for i, row in enumerate(policy):\n",
        "    for j, val in enumerate(row):\n",
        "      policy_vis[i,j]=transform[tuple(policy[i,j])]\n",
        "\n",
        "  print(policy_vis)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def episode_visualization(episode):\n",
        "  # Create a simple representation for a policy\n",
        "  episode_vis=np.full((10,10), \" \")\n",
        "  for i, e in enumerate(episode):\n",
        "      episode_vis[e[1][0],e[1][1]]=\"X\"\n",
        "\n",
        "  print(episode_vis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sW76hrQKo6K"
      },
      "source": [
        "# 1. Dynamic Programming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HezToupC5-55"
      },
      "source": [
        "## 1.1. Policy Iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wJ5Z6Pc5Jf5"
      },
      "outputs": [],
      "source": [
        "\n",
        "#V=np.zeros(w.rewards.shape)\n",
        "V = np.random.normal(0,1,w.rewards.shape)\n",
        "length, width=w.rewards.shape[0]-1, w.rewards.shape[1]-1\n",
        "bounds=(0,0,length, width)\n",
        "\n",
        "action_space=[(0,1),(0,-1),(1,0),(-1,0)]\n",
        "s=(0,0)\n",
        "\n",
        "print(V,V[s], tuple(a+b for a,b in zip(s,action_space[0])), bounds)\n",
        "\n",
        "# Create an arbitrary detterministic policy (we let the environment explore for now)\n",
        "action_space_size=len(action_space)\n",
        "\n",
        "random_indices=np.random.choice(action_space_size, size=[length+1,width+1])\n",
        "policy = np.array(action_space)[random_indices]\n",
        "\n",
        "print(\"Policy:\", policy[0,0])\n",
        "\n",
        "\n",
        "def calc_expected_value_n_states(V,w,action,s):\n",
        "\n",
        "  S=0\n",
        "  new_s= tuple(a+b if (a+b<=9) and (a+b>=0) else a for a,b in zip(s, action))\n",
        "\n",
        "\n",
        "  if tuple(action)==(0,1) or tuple(action)==(0,-1):\n",
        "    new_s_adj1= tuple([new_s[0]+1 if 0<=new_s[0]+1<=9 else new_s[0], new_s[1]])\n",
        "    new_s_adj2= tuple([new_s[0]-1 if 0<=new_s[0]-1<=9 else new_s[0], new_s[1]])\n",
        "\n",
        "  if tuple(action)==(1,0) or tuple(action)==(-1,0):\n",
        "    new_s_adj1= tuple([new_s[0], new_s[1]+1 if 0<=new_s[1]+1<=9 else new_s[1]])\n",
        "    new_s_adj2= tuple([new_s[0], new_s[1]-1 if 0<=new_s[1]-1<=9 else new_s[1]])\n",
        "\n",
        "\n",
        "  S=w.p_env* (w.rewards[new_s] + gamma*V[new_s])+((1-w.p_env)/2)* (w.rewards[new_s_adj1] +  gamma*V[new_s_adj1]) \\\n",
        "   + ((1-w.p_env)/2)* (w.rewards[new_s_adj2] + gamma*V[new_s_adj2])\n",
        "\n",
        "\n",
        "  return S, (new_s, new_s_adj1, new_s_adj2)\n",
        "\n",
        "\n",
        "theta= 0.0000001\n",
        "for ii in range(100):\n",
        "  print(\"\\n\\n ------------------------- \",ii,\" ------------------------------\")\n",
        "  delta=30\n",
        "\n",
        "  cnt=0\n",
        "  # Policy Evaluation:\n",
        "  while delta> theta:\n",
        "\n",
        "    cnt+=1\n",
        "    #print(\"cnt\",cnt, V[8,9])\n",
        "\n",
        "    delta=0\n",
        "\n",
        "    for i, row in enumerate(V):\n",
        "      for j , element in enumerate(row):\n",
        "\n",
        "        s=(i,j)\n",
        "        v_old= V[s]\n",
        "\n",
        "        if s!=w.s_terminal[0]:\n",
        "          V[s],_=calc_expected_value_n_states(V, w, policy[s], s)\n",
        "        else:\n",
        "          V[s]=0\n",
        "\n",
        "        delta = np.max([delta, np.abs(V[s]-v_old)])\n",
        "\n",
        "        #print(w.s_terminal,s, V[s])\n",
        "\n",
        "  #print(\"V is: \",V)\n",
        "  #policy_visualization(policy)\n",
        "\n",
        "  ## Policy Improvement:\n",
        "  policy_stable=True\n",
        "  for i, row in enumerate(policy):\n",
        "      for j , element in enumerate(row):\n",
        "          s=(i,j)\n",
        "          old_action= policy[s].copy()\n",
        "          expected_values=[]\n",
        "          for action in action_space:\n",
        "            val,_ = calc_expected_value_n_states(V,w,action,s)\n",
        "            expected_values.append(val)\n",
        "\n",
        "          max_val= max(expected_values)\n",
        "          chosen_index=expected_values.index(max_val)\n",
        "          chosen_action = action_space[chosen_index]\n",
        "          policy[s]=chosen_action\n",
        "          #print(\"Chosen action\",chosen_action)\n",
        "\n",
        "          if tuple(old_action) != chosen_action:\n",
        "            policy_stable =False\n",
        "\n",
        "\n",
        "\n",
        "  # Create the heatmap\n",
        "  V_rounded= np.round(V,1)\n",
        "\n",
        "  sns.heatmap(w.rewards, annot=True, cmap='viridis')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  sns.heatmap(V_rounded,  cmap='viridis')\n",
        "\n",
        "  # Add titles and labels as needed\n",
        "  plt.title('Heatmap of 2D Array')\n",
        "  plt.xlabel('X-axis Label')\n",
        "  plt.ylabel('Y-axis Label')\n",
        "\n",
        "  # Show the plot\n",
        "  plt.show()\n",
        "  policy_visualization(policy)\n",
        "  print(V_rounded)\n",
        "  #breakpoint()\n",
        "  if policy_stable:\n",
        "    break\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaWYoZmVBxA-"
      },
      "source": [
        "## 1.2. Value *Iteration*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNT1Tvy_Bwsj"
      },
      "outputs": [],
      "source": [
        "#V=np.zeros(w.rewards.shape)\n",
        "V = np.random.normal(0,1,w.rewards.shape)\n",
        "length, width=w.rewards.shape[0]-1, w.rewards.shape[1]-1\n",
        "bounds=(0,0,length, width)\n",
        "\n",
        "action_space=[(0,1),(0,-1),(1,0),(-1,0)]\n",
        "s=(0,0)\n",
        "\n",
        "print(V,V[s], tuple(a+b for a,b in zip(s,action_space[0])), bounds)\n",
        "\n",
        "# Create an arbitrary detterministic policy (we let the environment explore for now)\n",
        "action_space_size=len(action_space)\n",
        "\n",
        "random_indices=np.random.choice(action_space_size, size=[length+1,width+1])\n",
        "policy = np.array(action_space)[random_indices]\n",
        "\n",
        "print(\"Policy:\", policy[0,0])\n",
        "\n",
        "\n",
        "def calc_expected_value_n_states(V,w,action,s):\n",
        "\n",
        "  S=0\n",
        "  new_s= tuple(a+b if (a+b<=9) and (a+b>=0) else a for a,b in zip(s, action))\n",
        "\n",
        "\n",
        "  if tuple(action)==(0,1) or tuple(action)==(0,-1):\n",
        "    new_s_adj1= tuple([new_s[0]+1 if 0<=new_s[0]+1<=9 else new_s[0], new_s[1]])\n",
        "    new_s_adj2= tuple([new_s[0]-1 if 0<=new_s[0]-1<=9 else new_s[0], new_s[1]])\n",
        "\n",
        "  if tuple(action)==(1,0) or tuple(action)==(-1,0):\n",
        "    new_s_adj1= tuple([new_s[0], new_s[1]+1 if 0<=new_s[1]+1<=9 else new_s[1]])\n",
        "    new_s_adj2= tuple([new_s[0], new_s[1]-1 if 0<=new_s[1]-1<=9 else new_s[1]])\n",
        "\n",
        "\n",
        "  S=w.p_env* (w.rewards[new_s] + gamma*V[new_s])+((1-w.p_env)/2)* (w.rewards[new_s_adj1] +  gamma*V[new_s_adj1]) \\\n",
        "   + ((1-w.p_env)/2)* (w.rewards[new_s_adj2] + gamma*V[new_s_adj2])\n",
        "\n",
        "\n",
        "  return S, (new_s, new_s_adj1, new_s_adj2)\n",
        "\n",
        "\n",
        "theta= 0.0001\n",
        "delta=30\n",
        "ii=0\n",
        "\n",
        "# Policy Evaluation:\n",
        "while delta> theta:\n",
        "  ii+=1\n",
        "  print(\"\\n\\n ------------------------- \",ii,\" ------------------------------\")\n",
        "\n",
        "\n",
        "  delta=0\n",
        "  for i, row in enumerate(V):\n",
        "    for j , element in enumerate(row):\n",
        "      s=(i,j)\n",
        "      v_old= V[s]\n",
        "      if s!=w.s_terminal[0]:\n",
        "        expected_values=[]\n",
        "        for action in action_space:\n",
        "          val,_ = calc_expected_value_n_states(V,w,action,s)\n",
        "          expected_values.append(val)\n",
        "\n",
        "        max_val= max(expected_values)\n",
        "        V[s]=max_val\n",
        "      else:\n",
        "        V[s]=0\n",
        "\n",
        "      delta = np.max([delta, np.abs(V[s]-v_old)])\n",
        "\n",
        "        #print(w.s_terminal,s, V[s])\n",
        "\n",
        "\n",
        "  # Create the heatmap\n",
        "  V_rounded= np.round(V,1)\n",
        "\n",
        "  sns.heatmap(w.rewards, annot=True, cmap='viridis')\n",
        "  plt.show()\n",
        "\n",
        "  sns.heatmap(V_rounded,  cmap='viridis')\n",
        "\n",
        "  # Add titles and labels as needed\n",
        "  plt.title('Heatmap of 2D Array')\n",
        "  plt.xlabel('X-axis Label')\n",
        "  plt.ylabel('Y-axis Label')\n",
        "\n",
        "  # Show the plot\n",
        "  plt.show()\n",
        "  print(V_rounded)\n",
        "\n",
        "\n",
        "# Based on the value function choose the policy:\n",
        "for i, row in enumerate(V):\n",
        "    for j , element in enumerate(row):\n",
        "      s=(i,j)\n",
        "      expected_values=[]\n",
        "      for action in action_space:\n",
        "        val,_ = calc_expected_value_n_states(V,w,action,s)\n",
        "        expected_values.append(val)\n",
        "\n",
        "      max_val= max(expected_values)\n",
        "      chosen_index=expected_values.index(max_val)\n",
        "      chosen_action = action_space[chosen_index]\n",
        "      policy[s]=chosen_action\n",
        "\n",
        "\n",
        "\n",
        "policy_visualization(policy)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddO7d7Xi-DWr"
      },
      "source": [
        "## 2 Monte Carlo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xLqf9bIIklY"
      },
      "outputs": [],
      "source": [
        "def initialize_returns(world):\n",
        "  returns={}\n",
        "  for i,row in enumerate(world.rewards):\n",
        "    for j, _ in enumerate(row):\n",
        "      key=str(i)+\"_\"+str(j)\n",
        "      for a in world.a_space:\n",
        "        a_key=\"_\"+str(a[0])+\"_\"+str(a[1])\n",
        "        returns[key+a_key]=[]\n",
        "\n",
        "  return returns\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def generate_episode(world, policy, Q, max_cnt=100000, epsilon=0.2, mode=\"ES\"):\n",
        "  #handle episodes that are too large\n",
        "  cnt=max_cnt\n",
        "  episode=[]\n",
        "  outer_cnt=0\n",
        "  while cnt>=max_cnt or len(episode)<=2:\n",
        "    cnt=0\n",
        "    # generate a random starting state\n",
        "    if mode==\"ES\":\n",
        "      s0=tuple(np.random.randint((10,10)))\n",
        "    else:\n",
        "      s0=world.s_start[0]\n",
        "\n",
        "\n",
        "    episode=[(None,s0,tuple(policy[s0]))]\n",
        "    s=s0\n",
        "    episode_history={key:0 for key in Q.keys()}\n",
        "\n",
        "    while (s not in world.s_terminal) and cnt<=max_cnt:\n",
        "      old_s=s\n",
        "      s,a,r=calc_transition(world,policy,s, epsilon=epsilon)\n",
        "\n",
        "      if (old_s[0]==0) and (tuple(a)==(-1,0)):\n",
        "        a=(1,0)\n",
        "\n",
        "      if ((old_s[1]==0) and tuple(a)==(0,-1)):\n",
        "        a=(0,1)\n",
        "        #print(\"HEY\",s,a)\n",
        "\n",
        "\n",
        "\n",
        "      #print(s,a)\n",
        "      episode.append((r,old_s,a))\n",
        "      episode_history[f\"{old_s[0]}_{old_s[1]}_{a[0]}_{a[1]}\"]+=1\n",
        "      #print(s,a)\n",
        "\n",
        "      cnt+=1\n",
        "\n",
        "    outer_cnt+=1\n",
        "\n",
        "    if outer_cnt>1000:\n",
        "      print(\"BADPOLICY Reinit Policy\")\n",
        "      policy=initialize_policy(world)\n",
        "\n",
        "    #print(\"\\n\\n\")\n",
        "    #print(outer_cnt, s0,s, len(episode))#,episode)\n",
        "    #policy_visualization(policy)\n",
        "    #episode_visualization(episode)\n",
        "\n",
        "\n",
        "\n",
        "  return episode, episode_history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enws2yVgilhS"
      },
      "source": [
        "### 2.1 Exploring Starts & on-policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gkMLjnFAnld"
      },
      "outputs": [],
      "source": [
        "\n",
        "Q = initialize_Q(w, True)\n",
        "policy=initialize_policy(w,init=\"random\")# \"down\"\n",
        "returns=initialize_returns(w)\n",
        "\n",
        "step_size=1000\n",
        "val_epsilon=0.4\n",
        "print(Q[\"0_0_1_0\"])\n",
        "max_Q,min_Q=np.zeros(w.rewards.shape),np.zeros(w.rewards.shape)\n",
        "\n",
        "\n",
        "for ii in range(100000):\n",
        "  episode, episode_history=generate_episode(w, policy, Q, max_cnt=step_size,mode=\"on-policy\", epsilon=val_epsilon)\n",
        "  #print(\"\\n\\n\",ii, len(episode))\n",
        "  #create_Q_heatmap(max_Q, min_Q)\n",
        "  #policy_visualization(policy)\n",
        "  #episode_visualization(episode)\n",
        "\n",
        "  if ii % 1000==0:\n",
        "    create_Q_heatmap(max_Q, min_Q)\n",
        "    policy_visualization(policy)\n",
        "    step_size+=1000\n",
        "    if val_epsilon>0.15:\n",
        "        val_epsilon-=0.05\n",
        "\n",
        "    #if val_epsilon<0:\n",
        "    #  val_epsilon=0\n",
        "    print(ii,\"Policy Simulation:\",w.p_env,val_epsilon, step_size)\n",
        "\n",
        "  #print(Q[\"0_0_0_-1\"],Q[\"0_0_-1_0\"],Q[\"0_0_1_0\"],len(episode),episode)\n",
        "  G=0\n",
        "  for t  in range(len(episode)-2, 0,-1 ):\n",
        "    G=w.gamma*G+episode[t+1][0]\n",
        "    key=f\"{episode[t][1][0]}_{episode[t][1][1]}_{episode[t][2][0]}_{episode[t][2][1]}\"\n",
        "    episode_history[key]=episode_history[key]-1\n",
        "    #print(key,episode_history[key])\n",
        "    if episode_history[key]<=0:\n",
        "      returns[key].append(G)\n",
        "      Q[key]=sum(returns[key])/len(returns[key])\n",
        "\n",
        "      old_policy=policy.copy()\n",
        "      max_Q,policy,min_Q=calc_min_max_Q_n_greedy(Q,w,policy)\n",
        "\n",
        "      if not np.array_equal(old_policy,policy):\n",
        "        print(ii, \"policy changed:\")\n",
        "      # for i, row in enumerate(old_policy):\n",
        "      #   for j, e in enumerate(row):\n",
        "      #     if tuple(old_policy[i,j])!=tuple(policy[i,j]):\n",
        "      #        print(f\"({i}_{j}): old:{old_policy[i,j]}-> {policy[i,j]}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(returns,\"\\n\",Q)\n",
        "create_Q_heatmap(max_Q, min_Q)\n",
        "policy_visualization(policy)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEKuv_48Mj-5",
        "outputId": "fd276107-a4e5-45a1-b172-05c60bae625d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-65.82181905831247 32790 [-20.000000000000014, -20.000000000000014, -20.000000000000014, -20.000000000000014, -20.000000000000014]\n"
          ]
        }
      ],
      "source": [
        "ss=\"0_1_0_1\"\n",
        "print(Q[ss],len(returns[ss]),returns[ss][-5:] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VipRrk1AJFlX"
      },
      "source": [
        "### 2.2 Off-Policy (Not 100% its correct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKykOABJJMTq"
      },
      "outputs": [],
      "source": [
        "Q = initialize_Q(w, True)\n",
        "C = initialize_Q(w, True)\n",
        "Counters=initialize_Q(w, True)\n",
        "\n",
        "\n",
        "pi_policy=initialize_policy(w,init=\"random\")# \"down\"\n",
        "max_Q, pi_policy, min_Q= calc_min_max_Q_n_greedy(Q,w, pi_policy)\n",
        "\n",
        "print(pi_policy.shape)\n",
        "\n",
        "b_policy=initialize_policy(w,init=\"random\")\n",
        "\n",
        "\n",
        "\n",
        "returns=initialize_returns(w)\n",
        "\n",
        "step_size=1000\n",
        "\n",
        "pi_epsilon=0.05 # the simler strategy\n",
        "b_epsilon=0.5\n",
        "\n",
        "max_Q,min_Q=np.zeros(w.rewards.shape),np.zeros(w.rewards.shape)\n",
        "\n",
        "\n",
        "\n",
        "for ii in range(100000):\n",
        "  episode, episode_history=generate_episode(w, b_policy, Q, max_cnt=step_size,mode=\"on-policy\", epsilon=b_epsilon)\n",
        "  #print(ii, len(episode))\n",
        "  b_policy=initialize_policy(w,init=\"random\")\n",
        "\n",
        "\n",
        "  if ii % 1000==0:\n",
        "    create_Q_heatmap(max_Q, min_Q)\n",
        "    policy_visualization(pi_policy)\n",
        "    step_size+=1000\n",
        "\n",
        "    print(ii,\"Policy Simulation:\",w.p_env, step_size, {k:round(v,2) for k,v in Q.items()})\n",
        "    print(\"Counters\", Counters)\n",
        "\n",
        "  W=1\n",
        "  G=0\n",
        "  for t  in range(len(episode)-2, 0,-1 ):\n",
        "\n",
        "    G=w.gamma*G+episode[t+1][0]\n",
        "    key=f\"{episode[t][1][0]}_{episode[t][1][1]}_{episode[t][2][0]}_{episode[t][2][1]}\"\n",
        "\n",
        "    C[key]+=W\n",
        "    Counters[key]+=1\n",
        "    if C[key]==0:\n",
        "      print(\"C of key 0.. Breaking\")\n",
        "      break\n",
        "\n",
        "    Q[key]+=(W/C[key])*(G-Q[key])\n",
        "\n",
        "    old_pi_policy=pi_policy.copy()\n",
        "    max_Q, pi_policy, min_Q= calc_min_max_Q_n_greedy(Q,w, pi_policy)\n",
        "    policy_changed(old_pi_policy,pi_policy)\n",
        "\n",
        "\n",
        "    # Calculate importance sampling probability ratios per policy\n",
        "    current_action=(episode[t][2][0],episode[t][2][1])\n",
        "\n",
        "    greedy_action=tuple(pi_policy[episode[t][1][0], episode[t][1][1]])\n",
        "    behaviour_action=tuple(b_policy[episode[t][1][0], episode[t][1][1]])\n",
        "\n",
        "\n",
        "\n",
        "    if current_action==behaviour_action:\n",
        "      p_b=1-b_epsilon\n",
        "    else:\n",
        "      p_b=b_epsilon/3\n",
        "\n",
        "    if current_action==greedy_action:\n",
        "      p_pi=1-pi_epsilon\n",
        "    else:\n",
        "      p_pi=pi_epsilon/3\n",
        "\n",
        "\n",
        "    W=W*(p_pi/p_b)\n",
        "    #print(\"W\",key, W/C[key])\n",
        "\n",
        "    # only if pi_policy i deterministic\n",
        "    #if current_action!=greedy_action:\n",
        "    #  #print(\"break episofe\",t, len(episode))\n",
        "    #  break\n",
        "\n",
        "    #W=W*(1/p_b)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(returns,\"\\n\",Q)\n",
        "create_Q_heatmap(max_Q, min_Q)\n",
        "policy_visualization(policy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Za9axqk68Lt4",
        "outputId": "25a213a8-136b-4bd0-af04-5b4e65fb3765"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-9.85622450678161 0 []\n"
          ]
        }
      ],
      "source": [
        "ss=\"7_1_0_1\"\n",
        "print(Q[ss],len(returns[ss]),returns[ss][-5:] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC9yc2LgqLOz"
      },
      "source": [
        "## TD Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFJmMajiWoOd"
      },
      "outputs": [],
      "source": [
        "def transition_handler(world,policy,s,epsilon, display=False):\n",
        "\n",
        "  new_s,a,r=calc_transition(world,policy,s, epsilon=epsilon, display=display)\n",
        "  if (s[0]==0) and (tuple(a)==(-1,0)):\n",
        "    a=(0,0)\n",
        "\n",
        "\n",
        "  if ((s[1]==0) and tuple(a)==(0,-1)):\n",
        "    a=(0,0)\n",
        "\n",
        "  if ((s[0]==9) and tuple(a)==(1,0)):\n",
        "    a=(0,0)\n",
        "\n",
        "\n",
        "  if ((s[1]==9) and tuple(a)==(0,1)):\n",
        "    a=(0,0)\n",
        "\n",
        "  #print(new_s,a,r)\n",
        "\n",
        "\n",
        "  return new_s, a, r\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AenFRq8CRKH"
      },
      "source": [
        "### All: On-Policy (Sarsa), off-policy (Q-learning), expected sarsa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfvauPdrApkm"
      },
      "outputs": [],
      "source": [
        "Q=initialize_Q(w, True)\n",
        "policy=initialize_policy(w,init=\"random\")# \"down\"\n",
        "max_Q, policy, min_Q= calc_min_max_Q_n_greedy(Q,w, policy)\n",
        "policy_visualization(policy)\n",
        "\n",
        "\n",
        "\n",
        "mode=\"ES\"# \"Q\" for Q-learning, \"sarsa for sarsa\", else expected sarsa\n",
        "print(\"Mode:\",mode)\n",
        "\n",
        "epsilon=0.15\n",
        "max_cnt=10000\n",
        "alpha=0.01\n",
        "\n",
        "for i in range(20000):\n",
        "\n",
        "  if i%100==0:\n",
        "    print(\"Episode: \",i)\n",
        "    if i%1000==0:\n",
        "      create_Q_heatmap(max_Q, min_Q)\n",
        "      policy_visualization(policy)\n",
        "\n",
        "\n",
        "  max_Q, policy, min_Q= calc_min_max_Q_n_greedy(Q,w, policy)\n",
        "\n",
        "  s=w.s_start[0]\n",
        "  new_s,a,r=transition_handler(w,policy,s, epsilon=epsilon)\n",
        "\n",
        "  cnt=0\n",
        "  while (new_s not in w.s_terminal) and cnt<=max_cnt:\n",
        "    key = f\"{s[0]}_{s[1]}_{a[0]}_{a[1]}\"\n",
        "\n",
        "    s=new_s\n",
        "    new_s,new_a,new_r=transition_handler(w,policy, s, epsilon=epsilon)\n",
        "\n",
        "\n",
        "    if mode==\"sarsa\":\n",
        "      new_key = f\"{s[0]}_{s[1]}_{new_a[0]}_{new_a[1]}\"\n",
        "      Q[key]+=alpha*(r+w.gamma*Q[new_key] - Q[key])\n",
        "    elif mode==\"Q\":\n",
        "      c_l=[key for key in Q.keys() if key.startswith(f\"{s[0]}_{s[1]}\")]\n",
        "      Qs=[Q[c_key] for c_key in c_l]\n",
        "\n",
        "      Q[key]+=alpha*(r+w.gamma* max(Qs) - Q[key])\n",
        "\n",
        "    else: # expected sarsa\n",
        "      chosen_key=f\"{s[0]}_{s[1]}_{policy[s][0]}_{policy[s][1]}\"\n",
        "      c_l=[k for k in Q.keys() if k.startswith(f\"{s[0]}_{s[1]}\")]\n",
        "      S=0\n",
        "      for k in c_l:\n",
        "        if k!=chosen_key:\n",
        "          S+=(epsilon/(len(c_l)-1))*Q[k]\n",
        "        else:\n",
        "          S+=(1-epsilon)*Q[k]\n",
        "\n",
        "      Q[key]+=alpha*(r+w.gamma*S - Q[key])\n",
        "\n",
        "\n",
        "\n",
        "    a,r= new_a, new_r\n",
        "\n",
        "\n",
        "    cnt+=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbvYvR4AfXrQ"
      },
      "source": [
        "##  N-Step *Bootstrapping*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_SBkTqngVvB"
      },
      "source": [
        "### On-Policy (n-step Sarsa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da-1THZ6goYE"
      },
      "outputs": [],
      "source": [
        "Q=initialize_Q(w, True)\n",
        "policy=initialize_policy(w,init=\"random\")# \"down\"\n",
        "max_Q, _, min_Q= calc_min_max_Q_n_greedy(Q,w, policy)\n",
        "policy=initialize_policy(w,init=\"random\")# \"down\"\n",
        "\n",
        "init_mode=\"random\"# random\n",
        "\n",
        "\n",
        "alpha=0.01\n",
        "epsilon=0.15\n",
        "n=5\n",
        "\n",
        "\n",
        "for i in range(30000):\n",
        "\n",
        "  if i%100==0:\n",
        "    print(\"Episode: \",i)\n",
        "    if i%1000==0:\n",
        "      create_Q_heatmap(max_Q, min_Q)\n",
        "      policy_visualization(policy)\n",
        "\n",
        "  if init_mode==\"start\":\n",
        "    s=w.s_start[0]\n",
        "  else:\n",
        "    s=(random.randint(0,9),random.randint(0,9))\n",
        "    while s==w.s_terminal:\n",
        "     s=(random.randint(0,9),random.randint(0,9))\n",
        "\n",
        "  T=10000000000\n",
        "  t=0\n",
        "  episode=[]\n",
        "  new_s,a,r =transition_handler(w,policy,s,epsilon)\n",
        "  while 1:\n",
        "\n",
        "    if t<T:\n",
        "        #print(new_s)\n",
        "        episode.append((s,a,r))\n",
        "        if new_s in w.s_terminal:\n",
        "          T=t+1\n",
        "        else:\n",
        "          s=new_s\n",
        "          new_s,a,r =transition_handler(w,policy,s,epsilon)\n",
        "\n",
        "    T_small= t-n+1\n",
        "\n",
        "    if T_small>=0:\n",
        "      G=0\n",
        "      for tt in range(T_small+1, min(T_small+n+1,T+1)):\n",
        "        G+=(w.gamma**(tt-T_small-1))*episode[tt-1][2]\n",
        "        #print(T_small+1, tt, T_small+n+1, T+1)\n",
        "\n",
        "\n",
        "      if T_small+n<T:\n",
        "        current_key=f\"{s[0]}_{s[1]}_{a[0]}_{a[1]}\"\n",
        "        G+=(w.gamma**n)*Q[current_key]\n",
        "\n",
        "      update_key=f\"{episode[T_small][0][0]}_{episode[T_small][0][1]}_{episode[T_small][1][0]}_{episode[T_small][1][1]}\"\n",
        "      Q[update_key]+=alpha*(G-Q[update_key])\n",
        "      max_Q, policy, min_Q= calc_min_max_Q_n_greedy(Q,w, policy)\n",
        "\n",
        "\n",
        "    #print(\"Current\",s,a,\"Episode:\",episode)\n",
        "    t+=1\n",
        "    if T_small == T-1:\n",
        "      break\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po4Mp-IUzRHf"
      },
      "source": [
        "### Off-Policy (Too much variance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tqtN0ImzUCk"
      },
      "outputs": [],
      "source": [
        "Q=initialize_Q(w, True)\n",
        "pi_policy=initialize_policy(w,init=\"random\")# \"down\"\n",
        "max_Q, _, min_Q= calc_min_max_Q_n_greedy(Q,w, pi_policy)\n",
        "#pi_policy=initialize_policy(w,init=\"random\")# \"down\"\n",
        "\n",
        "b_policy=initialize_policy(w,init=\"random\")# \"down\"\n",
        "\n",
        "init_mode=\"random\"#\"start\"# random\n",
        "\n",
        "\n",
        "alpha=0.01\n",
        "pi_epsilon=0.15\n",
        "b_epsilon=0.5\n",
        "n=5\n",
        "w.gamma=0.9\n",
        "\n",
        "for i in range(30000):\n",
        "\n",
        "  if i%1==0:\n",
        "    print(\"Episode: \",i)\n",
        "    if i%100==0:\n",
        "      create_Q_heatmap(max_Q, min_Q)\n",
        "      policy_visualization(pi_policy)\n",
        "\n",
        "  if init_mode==\"start\":\n",
        "    s=w.s_start[0]\n",
        "  else:\n",
        "    s=(random.randint(0,9),random.randint(0,9))\n",
        "    while s==w.s_terminal:\n",
        "     s=(random.randint(0,9),random.randint(0,9))\n",
        "\n",
        "  T=10000000000\n",
        "  t=0\n",
        "  episode=[]\n",
        "  new_s,a,r =transition_handler(w,b_policy,s,b_epsilon)\n",
        "  while 1:\n",
        "\n",
        "    if t<T:\n",
        "        #print(new_s)\n",
        "        episode.append((s,a,r))\n",
        "        if new_s in w.s_terminal:\n",
        "          T=t+1\n",
        "        else:\n",
        "          s=new_s\n",
        "          new_s,a,r =transition_handler(w,b_policy,s,b_epsilon)\n",
        "\n",
        "    T_small= t-n+1\n",
        "\n",
        "    if T_small>=0:\n",
        "      ro=1\n",
        "      for tt in range(T_small+1, min(T_small+n,T)):\n",
        "        current_action=tuple(episode[tt-1][1])\n",
        "\n",
        "        greedy_action=tuple(pi_policy[episode[tt-1][0][0], episode[tt-1][0][1]])\n",
        "        behaviour_action=tuple(b_policy[episode[tt-1][0][0], episode[tt-1][0][1]])\n",
        "\n",
        "        if current_action==behaviour_action:\n",
        "          p_b=1-b_epsilon\n",
        "        else:\n",
        "          p_b=b_epsilon/4\n",
        "\n",
        "        if current_action==greedy_action:\n",
        "          p_pi=1-pi_epsilon\n",
        "        else:\n",
        "          p_pi=pi_epsilon/4\n",
        "\n",
        "\n",
        "        ro=ro*(p_pi/p_b)\n",
        "\n",
        "\n",
        "\n",
        "      G=0\n",
        "      for tt in range(T_small+1, min(T_small+n+1,T+1)):\n",
        "        G+=(w.gamma**(tt-T_small-1))*episode[tt-1][2]\n",
        "        #print(T_small+1, tt, T_small+n+1, T+1)\n",
        "\n",
        "\n",
        "      if T_small+n<T:\n",
        "        current_key=f\"{s[0]}_{s[1]}_{a[0]}_{a[1]}\"\n",
        "        G+=(w.gamma**n)*Q[current_key]\n",
        "\n",
        "      update_key=f\"{episode[T_small][0][0]}_{episode[T_small][0][1]}_{episode[T_small][1][0]}_{episode[T_small][1][1]}\"\n",
        "\n",
        "\n",
        "      Q[update_key]+=alpha*ro*(G-Q[update_key])\n",
        "      max_Q, pi_policy, min_Q= calc_min_max_Q_n_greedy(Q,w, pi_policy)\n",
        "\n",
        "      #print(ro,G,Q[\"3_3_1_0\"])\n",
        "\n",
        "    #print(\"Current\",s,a,\"Episode:\",episode)\n",
        "    t+=1\n",
        "    if T_small == T-1:\n",
        "      break\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pso9ZTGEdtqu"
      },
      "source": [
        ":### Off policy no importance sampling (tree backup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzuzrPzrd0yC"
      },
      "outputs": [],
      "source": [
        "Q=initialize_Q(w, True)\n",
        "pi_policy=initialize_policy(w,init=\"random\")# \"down\"\n",
        "max_Q, _, min_Q= calc_min_max_Q_n_greedy(Q,w, pi_policy)\n",
        "#pi_policy=initialize_policy(w,init=\"random\")# \"down\"\n",
        "\n",
        "b_policy=initialize_policy(w,init=\"random\")# \"down\"\n",
        "\n",
        "init_mode=\"start\"#\"start\"# random\n",
        "\n",
        "\n",
        "alpha=0.01\n",
        "pi_epsilon=0.1\n",
        "b_epsilon=0.3\n",
        "n=5\n",
        "episode=[]\n",
        "#w.gamma=0.95\n",
        "for i in range(30000):\n",
        "\n",
        "  if i%10==0:\n",
        "    print(\"Episode: \",i,len(episode))#, \"0_0_0_1\",Q['0_0_0_1'], \"0_0_1_0\",Q['0_0_1_0'])\n",
        "    if i%100==0:\n",
        "      create_Q_heatmap(max_Q, min_Q)\n",
        "      policy_visualization(pi_policy)\n",
        "\n",
        "  if init_mode==\"start\":\n",
        "    s=w.s_start[0]\n",
        "  else:\n",
        "    s=(random.randint(0,9),random.randint(0,9))\n",
        "    while s==w.s_terminal:\n",
        "     s=(random.randint(0,9),random.randint(0,9))\n",
        "\n",
        "  T=10000000000\n",
        "  t=0\n",
        "  episode=[]\n",
        "  new_s,a,r =transition_handler(w,b_policy,s,b_epsilon)\n",
        "  while 1:\n",
        "    if t<T:\n",
        "        #print(new_s)\n",
        "        episode.append((s,a,r))\n",
        "        if new_s in w.s_terminal:\n",
        "          T=t+1\n",
        "        else:\n",
        "          s=new_s\n",
        "          new_s,a,r =transition_handler(w,b_policy,s,b_epsilon)\n",
        "\n",
        "    T_small= t-n+1\n",
        "\n",
        "    if T_small>=0:\n",
        "      if t+1>=T:\n",
        "        G=r\n",
        "      else:\n",
        "        G=r\n",
        "        G1=0\n",
        "        for aa in w.a_space:\n",
        "          p_pi = pi_epsilon/4\n",
        "          if aa == tuple(pi_policy[s]):\n",
        "            p_pi=1-pi_epsilon\n",
        "          cond= ((s[0]==0) and (aa==(-1,0))) or ((s[1]==0) and (aa==(0,-1))) or ((s[0]==9) and (aa==(1,0))) or ((s[1]==9) and (aa==(0,1)))\n",
        "          if not cond:\n",
        "            key=f\"{s[0]}_{s[1]}_{aa[0]}_{aa[1]}\"\n",
        "            G1+=p_pi* Q[key]\n",
        "\n",
        "        G1=w.gamma*G1\n",
        "        G=r+G1\n",
        "\n",
        "      for tt in range(min(T-1, t), T_small,-1):\n",
        "        #print(i,\"new_s:\",new_s,\"s:\",s,\"tt:\",tt,\"Range:\",min(T-1, t), T_small,\"Episode Length:\",len(episode),\"Test:\",T_small,\"current\", episode[tt][0],\"To Update:\",episode[T_small][0],\"Episode:\",episode)\n",
        "        G1=0\n",
        "        p_pi_chosen=1-pi_epsilon\n",
        "        for aa in w.a_space:\n",
        "          p_pi_not_chosen = pi_epsilon/4\n",
        "          if aa != tuple(pi_policy[episode[tt][0]]):\n",
        "            key=f\"{episode[tt][0][0]}_{episode[tt][0][1]}_{aa[0]}_{aa[1]}\"\n",
        "            cond= ((episode[tt][0][0]==0) and (aa==(-1,0))) or ((episode[tt][0][1]==0) and (aa==(0,-1))) or ((episode[tt][0][0]==9) and (aa==(1,0))) or ((episode[tt][0][1]==9) and (aa==(0,1)))\n",
        "            if not cond:\n",
        "              G1+=p_pi_not_chosen* Q[key]\n",
        "\n",
        "\n",
        "        G=episode[tt][2] + w.gamma*G1 + w.gamma*p_pi_chosen*G\n",
        "\n",
        "\n",
        "\n",
        "      update_key=f\"{episode[T_small][0][0]}_{episode[T_small][0][1]}_{episode[T_small][1][0]}_{episode[T_small][1][1]}\"\n",
        "\n",
        "\n",
        "      Q[update_key]+=alpha*(G-Q[update_key])\n",
        "      max_Q, pi_policy, min_Q= calc_min_max_Q_n_greedy(Q,w, pi_policy)\n",
        "\n",
        "      #print(a,T_small,update_key,Q[update_key])\n",
        "\n",
        "    #print(\"Current\",s,a,\"Episode:\",episode)\n",
        "    t+=1\n",
        "    if T_small == T-1:\n",
        "      break\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOItVeV2LItGksdQFIbqR8i",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}